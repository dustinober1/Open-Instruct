# Open-Instruct Environment Configuration
# Copy this file to .env and fill in your values
# Do NOT commit .env to version control

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Choose ONE provider below (Ollama recommended for development)

# Option 1: Ollama (Local - Free)
# --------------------
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=deepseek-r1:1.5b

# Option 2: OpenAI (Cloud API)
# --------------------
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_MODEL=gpt-4

# Option 3: Anthropic (Cloud API)
# --------------------
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
# ANTHROPIC_MODEL=claude-3-sonnet-20240229

# =============================================================================
# Application Settings
# =============================================================================

# Server Configuration
PORT=8000
DEBUG=false
LOG_LEVEL=info

# Database Configuration
DATABASE_URL=sqlite:///./data/open_instruct.db

# Caching Configuration
CACHE_ENABLED=true
CACHE_TTL=604800  # 7 days in seconds

# Request Settings
REQUEST_TIMEOUT=60  # seconds

# =============================================================================
# DSPy Configuration
# =============================================================================
# DSPY_LLM_MODEL is derived from LLM_PROVIDER and provider-specific model
# No need to set this manually if using the above provider settings
